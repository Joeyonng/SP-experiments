{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "sys.path.append('./data')\n",
    "from Dataset import Dataset, dataset_config\n",
    "from StatisticalPrime import StatisticalPrime\n",
    "\n",
    "STATI_INDEX = {'coverage': 0, 'precision': 1, 'density_m': 2, 'density_n': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Examples of Prime Implicant and Statistical Prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset. The choices of the dataset are 'zoo', 'adult', 'lending', 'HELOC'\n",
    "dataset = Dataset(*dataset_config('lending'))\n",
    "# Balnce the dataset: both the training set and test set have the same number of positive and negative instances\n",
    "dataset.balance_dataset(0.8)\n",
    "print('Dataset train shape:', dataset.train_V.shape)\n",
    "print('Dataset test shape:', dataset.test_V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = StatisticalPrime(dataset)\n",
    "# Use espresso to derive prime implicants from the training set\n",
    "pure_primes = sp.espresso_primes()\n",
    "# Using default parameters to derive statistical primes from pure prime\n",
    "stat_primes, stat_statis = sp.statistical_primes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SPs for all unique labels in the dataset\n",
    "for y in np.unique(dataset.train_Y):\n",
    "    y_indices = np.where(stat_primes[:, -1] == np.power(2, y))[0]\n",
    "    \n",
    "    # Get SPs with high precisions \n",
    "    p_indices = stat_statis[y_indices][:, STATI_INDEX['precision']].argsort()[::-1][:100]\n",
    "    \n",
    "    # Get SPs with high precisions and low densities \n",
    "    d_indices = stat_statis[y_indices][p_indices][:, STATI_INDEX['density_m']].argsort()[:1]\n",
    "\n",
    "    for index in y_indices[p_indices[d_indices]]:\n",
    "        print(stat_statis[index])\n",
    "        print(StatisticalPrime.interpret(stat_primes[index], dataset.V_strs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Explanations with Dataset Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(*dataset_config('adult'))\n",
    "dataset.balance_dataset(0.8)\n",
    "print('Dataset train shape:', dataset.train_V.shape)\n",
    "print('Dataset test shape:', dataset.test_V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = StatisticalPrime(dataset)\n",
    "pure_primes = sp.espresso_primes()\n",
    "stat_primes, stat_statis = sp.statistical_primes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pV = np.power(2, dataset.train_V)\n",
    "    \n",
    "valid_indices = np.where(stat_statis[:, STATI_INDEX['precision']] > 0.90)[0]\n",
    "valid_primes = stat_primes[valid_indices]\n",
    "valid_statis = stat_statis[valid_indices]\n",
    "\n",
    "test_pV = np.power(2, dataset.test_V)\n",
    "results = []\n",
    "for prime in valid_primes:\n",
    "    results.append(StatisticalPrime.statistic(prime, test_pV, dataset.V_nums))\n",
    "\n",
    "print(valid_indices.shape[0])\n",
    "print(np.array(results).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from anchor import utils\n",
    "from anchor import anchor_tabular\n",
    "\n",
    "explainer = anchor_tabular.AnchorTabularExplainer(dataset.Y_strs, dataset.header[:-1], dataset.X, {index: features for index, features in enumerate(dataset.X_strs)})\n",
    "explainer.fit(dataset.train_X, dataset.train_Y, dataset.train_X, dataset.train_Y)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(explainer.encoder.transform(dataset.train_X), dataset.train_Y)\n",
    "predict_fn = lambda x: clf.predict(explainer.encoder.transform(x))\n",
    "print('Train', sklearn.metrics.accuracy_score(dataset.train_Y, predict_fn(dataset.train_X)))\n",
    "print('Test', sklearn.metrics.accuracy_score(dataset.test_Y, predict_fn(dataset.test_X)))\n",
    "\n",
    "test_pV = np.power(2, dataset.test_V)\n",
    "results = []\n",
    "for x in tqdm(dataset.train_X):\n",
    "    anchor = explainer.explain_instance(x, clf.predict, threshold=0.90)\n",
    "    \n",
    "    anchor_implicant = np.append(np.power(2, dataset.X_nums) - 1, np.power(2, predict_fn(x.reshape(1, -1))[0]))\n",
    "    anchor_implicant[anchor.features()] = np.power(2, x[anchor.features()])\n",
    "    \n",
    "    result = StatisticalPrime.statistic(anchor_implicant, test_pV, dataset.V_nums)\n",
    "    results.append(result)\n",
    "    \n",
    "print(np.array(results).mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Explanations as Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, primes, statis):\n",
    "    results = []\n",
    "    for x in tqdm(X):\n",
    "        px = np.power(2, x)\n",
    "\n",
    "        match_indices = StatisticalPrime.match(px, primes)\n",
    "        match_primes = primes[match_indices]\n",
    "        match_statis = statis[match_indices]\n",
    "        \n",
    "        valid_index = np.argmax(match_statis[:, STATI_INDEX['precision']])\n",
    "        result = int(np.log2(match_primes[valid_index][-1]))         \n",
    "        results.append(result)\n",
    "    return np.array(results)\n",
    "\n",
    "def score(X, Y, primes, statis):\n",
    "    predictions = predict(X, primes, statis)\n",
    "    results = predictions == Y\n",
    "    \n",
    "    right_indices = np.where(results == True)[0]\n",
    "    wrong_indices = np.where(results == False)[0]\n",
    "    accuracy =  right_indices.shape[0] / Y.shape[0]\n",
    "    \n",
    "    return accuracy, (right_indices, wrong_indices)\n",
    "\n",
    "def ref_classifiers(verbose=True):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    def ref_score(clf):\n",
    "        train_X = dataset.train_X\n",
    "        train_Y = dataset.train_Y\n",
    "        test_X = dataset.test_X\n",
    "        test_Y = dataset.test_Y\n",
    "        \n",
    "        clf.fit(train_X, train_Y)\n",
    "        test_score = clf.score(test_X, test_Y)\n",
    "        if verbose:\n",
    "            print(clf.__class__)\n",
    "            print(test_score)\n",
    "        return test_score\n",
    "    \n",
    "    results = []\n",
    "    results.append(ref_score(LogisticRegression()))\n",
    "    results.append(ref_score(DecisionTreeClassifier()))\n",
    "    results.append(ref_score(RandomForestClassifier()))\n",
    "    results.append(ref_score(AdaBoostClassifier()))\n",
    "    results.append(ref_score(KNeighborsClassifier()))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "k = 5\n",
    "for name in ['adult', 'lending', 'heloc']:\n",
    "    dataset = Dataset(*dataset_config(name))\n",
    "    k_fold = dataset.k_fold_dataset(k)\n",
    "\n",
    "    scores = np.array([0] * 6).astype(float)\n",
    "    for train_indices, test_indices in k_fold:\n",
    "        dataset.get_dataset(train_indices, test_indices)\n",
    "        \n",
    "        sp = StatisticalPrime(dataset)\n",
    "        pure_primes = sp.espresso_primes()\n",
    "        stat_primes, stat_statis = sp.statistical_primes()\n",
    "\n",
    "        test_accuracy, _ = score(dataset.test_V[:, :-1], dataset.test_V[:, -1], stat_primes, stat_statis)\n",
    "        ref_accuracies = ref_classifiers(verbose=False)\n",
    "        scores += np.array([test_accuracy] + ref_accuracies)\n",
    "    \n",
    "    results.append(scores / k)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
